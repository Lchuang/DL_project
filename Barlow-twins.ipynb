{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Barlow-twins.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcbe3738",
        "outputId": "153d8f20-7aa0-4a5b-a3a4-50d1ea054449"
      },
      "source": [
        "!pip install pandas==1.3.1\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging, os\n",
        "logging.disable(logging.WARNING)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation, Input \n",
        "from tensorflow.keras.layers import Conv1D, Flatten, MaxPooling1D, UpSampling1D, Reshape, Concatenate, Multiply, Lambda\n",
        "from tensorflow.keras.backend import int_shape\n",
        "from keras import backend as bken\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import keras\n",
        "from keras import layers\n",
        "from tensorflow.keras.backend import int_shape\n",
        "import os\n",
        "from scipy.special import comb\n",
        "import lr_scheduler"
      ],
      "id": "fcbe3738",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas==1.3.1 in /usr/local/lib/python3.7/dist-packages (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.1) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.1) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.1) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.1) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8sllkwgSuGl"
      },
      "source": [
        "!wget -o pd_data_dp_subset.p https://ucf318623584668d8740ede597a1.dl.dropboxusercontent.com/cd/0/get/BaPWpCQuhyIKlS1VCIa39IJm_f8YpUzm7Ix5c4H57oShk37iLp4BMZ07rThcDnUIyzk_r857u-Vj76bdZYKWkpEOwMHpW5W1-snEr4WDjbUC_XE0mEJvOnlAXTP0YA1vMPiBhmlIEkSgCQkpaa0-rHWl/file# --show-progress\n",
        "!mv file pd_data_dp_subset.p"
      ],
      "id": "W8sllkwgSuGl",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gy8PojPgUvMr",
        "outputId": "7e674271-2760-4def-8103-89f28b5aba11"
      },
      "source": [
        "!mkdir checkpoints & mkdir checkpoints/model_01\n",
        "!unzip 0100.ckpt.zip -d checkpoints/model_01/"
      ],
      "id": "Gy8PojPgUvMr",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘checkpoints’: File exists\n",
            "Archive:  0100.ckpt.zip\n",
            "   creating: checkpoints/model_01/0100.ckpt/\n",
            "   creating: checkpoints/model_01/0100.ckpt/assets/\n",
            "   creating: checkpoints/model_01/0100.ckpt/variables/\n",
            " extracting: checkpoints/model_01/0100.ckpt/saved_model.pb  \n",
            " extracting: checkpoints/model_01/0100.ckpt/keras_metadata.pb  \n",
            " extracting: checkpoints/model_01/0100.ckpt/variables/variables.index  \n",
            " extracting: checkpoints/model_01/0100.ckpt/variables/variables.data-00000-of-00001  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16c5085e"
      },
      "source": [
        "### Read dataset"
      ],
      "id": "16c5085e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb9704b4"
      },
      "source": [
        "df = pd.read_pickle('pd_data_dp_subset.p')\n",
        "\n",
        "# ---- recast event_id to int\n",
        "df[\"event_id\"] = df[\"event_id\"].apply(lambda x:int(x))"
      ],
      "id": "cb9704b4",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5138ba64"
      },
      "source": [
        "### Header\n",
        "    1. P_wave_processed: pre-processed p-wave recordings, each of them has dim = (600, 3)\n",
        "    2. P_wave_amplitudes: max(amplitude) of raw p-wave recordings, each of them has dim = (3,)\n",
        "    3. azimuth: the azimuth of earthquake with respect to station\n",
        "    4. station_name: name of the station\n",
        "    5. station_network: netowrk code of the station\n",
        "    6. event_id: event id\n",
        "    7. depth: event depth\n",
        "    8. station_lat: station coordinate lat.\n",
        "    9. station_long: station coordinate long.\n",
        "    10. event_lat: event coordinate lat\n",
        "    11. event_long: event coordinate long\n",
        "    12. magnitude: event magnitude\n",
        "    13. P: phase arrival time at station\n",
        "    14. delta: event station distance in degree (1 degree ~= 111 km)"
      ],
      "id": "5138ba64"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "09500984",
        "outputId": "b537c1ac-39ea-42c9-aa70-fb2894ddaf94"
      },
      "source": [
        "df.head(3)"
      ],
      "id": "09500984",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>station_name</th>\n",
              "      <th>station_network</th>\n",
              "      <th>station_lat</th>\n",
              "      <th>station_long</th>\n",
              "      <th>event_id</th>\n",
              "      <th>event_time</th>\n",
              "      <th>event_lat</th>\n",
              "      <th>event_long</th>\n",
              "      <th>depth</th>\n",
              "      <th>P_wave_processed</th>\n",
              "      <th>azimuth</th>\n",
              "      <th>delta</th>\n",
              "      <th>phase</th>\n",
              "      <th>P</th>\n",
              "      <th>magnitude</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AAK</td>\n",
              "      <td>II</td>\n",
              "      <td>42.637500</td>\n",
              "      <td>74.494200</td>\n",
              "      <td>1725583</td>\n",
              "      <td>9.470968e+08</td>\n",
              "      <td>-9.131</td>\n",
              "      <td>109.579</td>\n",
              "      <td>35.0</td>\n",
              "      <td>[[4.5132976810436585e-17, 0.0, -5.126423482750...</td>\n",
              "      <td>330.887</td>\n",
              "      <td>60.666</td>\n",
              "      <td>P</td>\n",
              "      <td>9.470974e+08</td>\n",
              "      <td>5.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAK</td>\n",
              "      <td>KN</td>\n",
              "      <td>42.633301</td>\n",
              "      <td>74.494400</td>\n",
              "      <td>1725583</td>\n",
              "      <td>9.470968e+08</td>\n",
              "      <td>-9.131</td>\n",
              "      <td>109.579</td>\n",
              "      <td>35.0</td>\n",
              "      <td>[[-0.004384802184109918, 0.002776395732707534,...</td>\n",
              "      <td>330.887</td>\n",
              "      <td>60.666</td>\n",
              "      <td>P</td>\n",
              "      <td>9.470974e+08</td>\n",
              "      <td>5.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AML</td>\n",
              "      <td>KN</td>\n",
              "      <td>42.131100</td>\n",
              "      <td>73.694099</td>\n",
              "      <td>1725583</td>\n",
              "      <td>9.470968e+08</td>\n",
              "      <td>-9.131</td>\n",
              "      <td>109.579</td>\n",
              "      <td>35.0</td>\n",
              "      <td>[[-0.003072967942240945, 0.002691759337183582,...</td>\n",
              "      <td>329.993</td>\n",
              "      <td>60.675</td>\n",
              "      <td>P</td>\n",
              "      <td>9.470974e+08</td>\n",
              "      <td>5.6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  station_name station_network  station_lat  ...  phase             P  magnitude\n",
              "0          AAK              II    42.637500  ...      P  9.470974e+08        5.6\n",
              "1          AAK              KN    42.633301  ...      P  9.470974e+08        5.6\n",
              "2          AML              KN    42.131100  ...      P  9.470974e+08        5.6\n",
              "\n",
              "[3 rows x 15 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ba9d7be"
      },
      "source": [
        "### Some facts of the dataset"
      ],
      "id": "2ba9d7be"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7caba169",
        "outputId": "6a6a176e-b5ec-4a10-e4d1-d17e07362481"
      },
      "source": [
        "total_entries = df.shape[0]\n",
        "print(f\"total {total_entries} entries\")\n",
        "event_number = pd.unique(df.event_id).shape[0]\n",
        "print(f\"total {event_number} events\")\n",
        "station_number = pd.unique(df.station_name).shape[0]\n",
        "print(f\"total ~{station_number} stations\")\n",
        "min_distance = df.delta.min()\n",
        "max_distance = df.delta.max()\n",
        "median_distance = df.delta.median()\n",
        "print(f\"min station-event distance {min_distance:.3f} degree\")\n",
        "print(f\"max station-event distance {max_distance:.3f} degree\")\n",
        "print(f\"median station-event distance {median_distance:.3f} degree\")"
      ],
      "id": "7caba169",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 145677 entries\n",
            "total 1295 events\n",
            "total ~1574 stations\n",
            "min station-event distance 0.199 degree\n",
            "max station-event distance 90.866 degree\n",
            "median station-event distance 68.233 degree\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aec41e2e"
      },
      "source": [
        "### Split train and test set\n",
        "To prevent data leaking from one event to another one, we will split dataset by event"
      ],
      "id": "aec41e2e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "051966a5"
      },
      "source": [
        "# Group by event and aggregate relevant columns\n",
        "df_evt_grouped = df.groupby('event_id').agg(\n",
        "    {'station_name': lambda x:list(x),\n",
        "     'station_network': lambda x:list(x),\n",
        "     'station_lat': lambda x:list(x),\n",
        "     'station_long': lambda x:list(x),\n",
        "     'P_wave_processed': lambda x:np.asarray(list(x))\n",
        "    }).reset_index()\n",
        "df_evt_grouped[\"num_waves\"] = df_evt_grouped[\"P_wave_processed\"]\\\n",
        "                              .apply(lambda x: x.shape[0])\n",
        "\n",
        "# ---- drop events that only has one recording\n",
        "df_evt_grouped = df_evt_grouped[df_evt_grouped.num_waves>=2]"
      ],
      "id": "051966a5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "085bf425"
      },
      "source": [
        "# Split train and test sets\n",
        "train_set, test_set, _, _ = train_test_split(df_evt_grouped, \n",
        "                                             df_evt_grouped, \n",
        "                                             test_size=0.2, \n",
        "                                             random_state=42)"
      ],
      "id": "085bf425",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36a378bb",
        "outputId": "de81e8d0-5964-406f-a1d0-3efd602fa229"
      },
      "source": [
        "num_train_events = train_set.shape[0]\n",
        "num_train_waves = train_set.num_waves.sum()\n",
        "num_test_events = test_set.shape[0]\n",
        "num_test_waves = test_set.num_waves.sum()\n",
        "print(f\"Number of train event: {num_train_events}\")\n",
        "print(f\"Number of train waves: {num_train_waves}\")\n",
        "print(f\"Number of test event: {num_test_events}\")\n",
        "print(f\"Number of test waves: {num_test_waves}\")"
      ],
      "id": "36a378bb",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of train event: 1034\n",
            "Number of train waves: 117692\n",
            "Number of test event: 259\n",
            "Number of test waves: 27983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36c2ffae"
      },
      "source": [
        "### Data for  the Resnet style Autoencoder"
      ],
      "id": "36c2ffae"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74f177fc"
      },
      "source": [
        "df_train_set_flat = df_evt_grouped.explode(['P_wave_processed'])"
      ],
      "id": "74f177fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57db1d43"
      },
      "source": [
        "waves_train = np.asarray((df_train_set_flat.P_wave_processed.to_list()))"
      ],
      "id": "57db1d43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b383ac30"
      },
      "source": [
        "#df_evt_grouped[[\"event_id\",\"P_wave_processed\"]].explode(['P_wave_processed'])"
      ],
      "id": "b383ac30",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "861392e6"
      },
      "source": [
        "### Model 1. 1D-Resnet style autoencoder"
      ],
      "id": "861392e6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3e7713e"
      },
      "source": [
        "def identity_block(x, num_filters=64, kernel_size=5):\n",
        "    \n",
        "    # copy tensor to variable called x_skip\n",
        "    x_skip = x\n",
        "    \n",
        "    # Layer 1\n",
        "    x = tf.keras.layers.Conv1D(num_filters, kernel_size, padding = 'same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    \n",
        "    # Layer 2\n",
        "    x = tf.keras.layers.Conv1D(num_filters, kernel_size, padding = 'same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
        "    \n",
        "    # Add Residue\n",
        "    x = tf.keras.layers.Add()([x, x_skip])     \n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    \n",
        "    return x"
      ],
      "id": "c3e7713e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa7f399f"
      },
      "source": [
        "def reverse_identity_block(x, num_filters=64, kernel_size=5):\n",
        "    \n",
        "    # copy tensor to variable called x_skip\n",
        "    x_skip = x\n",
        "    \n",
        "    # Layer 1\n",
        "    x = tf.keras.layers.Conv1DTranspose(num_filters, kernel_size, padding = 'same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    \n",
        "    # Layer 2\n",
        "    x = tf.keras.layers.Conv1DTranspose(num_filters, kernel_size, padding = 'same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
        "    \n",
        "    # Add Residue\n",
        "    x = tf.keras.layers.Add()([x, x_skip])     \n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    \n",
        "    return x"
      ],
      "id": "aa7f399f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1a90cf3"
      },
      "source": [
        "def ResAutoencoder(x_input_shape=(600,3),\n",
        "                   encoding_dim=100,\n",
        "                   num_res_blocks=9, \n",
        "                   max_pool_itval=3,\n",
        "                   pooling_dim = 2):\n",
        "    \n",
        "    # ============================================ #\n",
        "    # ============= Define the ENCODER =========== #\n",
        "    # ============================================ #\n",
        "    inputs = keras.Input(shape=x_input_shape, name='encoder_input')\n",
        "    x = inputs\n",
        "\n",
        "    # ---- inital cov block\n",
        "    x = tf.keras.layers.Conv1D(32, 7, padding = 'same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    \n",
        "    # ---- Residual blocks\n",
        "    for r in range(1, num_res_blocks + 1):\n",
        "        \n",
        "        x = identity_block(x, num_filters=32, kernel_size=5)\n",
        "        \n",
        "        # --- apply maxpooling after every n res blocks\n",
        "        if r % max_pool_itval == 0:\n",
        "            x = tf.keras.layers.MaxPooling1D(pooling_dim, padding='same')(x)\n",
        "    \n",
        "    # ============================================ #\n",
        "    # ============= Define the LATENT  =========== #\n",
        "    # ============================================ #\n",
        "    shape_before_flatten = int_shape(x)[1:]\n",
        "    x = Flatten()(x)  \n",
        "    latent = Dense(encoding_dim, name='latent_layer')(x)\n",
        "    encoder = Model(inputs=inputs, outputs=latent, name='encoder')\n",
        "    \n",
        "    # ============================================ #\n",
        "    # ============= Define the DECODER =========== #\n",
        "    # ============================================ #\n",
        "    \n",
        "    # ---- reshape back to the dim before flatten\n",
        "    latent_inputs = Input(shape=(encoding_dim), name='decoder_input')\n",
        "    x = latent_inputs\n",
        "    x = Dense(units=np.prod(shape_before_flatten))(x)\n",
        "    x = Reshape(target_shape=shape_before_flatten)(x)\n",
        "    \n",
        "    # ---- Residual blocks\n",
        "    for r in range(0, num_res_blocks-1):\n",
        "        \n",
        "        # --- apply maxpooling after every n res blocks\n",
        "        \n",
        "        if r % max_pool_itval == 0:\n",
        "            x = tf.keras.layers.UpSampling1D(pooling_dim)(x)\n",
        "        \n",
        "        x = reverse_identity_block(x, num_filters=32, kernel_size=5)\n",
        "    \n",
        "    # ---- Final cov block\n",
        "    x = tf.keras.layers.Conv1DTranspose(32, 7, padding = 'same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    \n",
        "    # ---- Final layer\n",
        "    output = Dense(units=x_input_shape[1])(x)\n",
        "\n",
        "    # ---- Build and compile decoder    \n",
        "    decoder = Model(inputs=latent_inputs, outputs=output, name='decoder')\n",
        "    \n",
        "    # ---- combine encoder and decoder\n",
        "    autoencoder = Model(inputs=inputs, \n",
        "                        outputs=decoder(encoder(inputs)), \n",
        "                        name='autoencoder')\n",
        "    return autoencoder, encoder, decoder"
      ],
      "id": "b1a90cf3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a268cd13",
        "outputId": "4240568b-d8b4-4b25-aeca-bed476e68073"
      },
      "source": [
        "res_autoencoder, encoder, decoder = ResAutoencoder(x_input_shape=(600,3),\n",
        "                     encoding_dim=100,\n",
        "                     num_res_blocks=9, \n",
        "                     max_pool_itval=3,\n",
        "                     pooling_dim = 2)\n",
        "res_autoencoder.summary()\n",
        "#encoder.summary()\n",
        "#decoder.summary()"
      ],
      "id": "a268cd13",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"autoencoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_input (InputLayer)  [(None, 600, 3)]          0         \n",
            "                                                                 \n",
            " encoder (Functional)        (None, 100)               335972    \n",
            "                                                                 \n",
            " decoder (Functional)        (None, 600, 3)            334307    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 670,279\n",
            "Trainable params: 667,975\n",
            "Non-trainable params: 2,304\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dd916d6"
      },
      "source": [
        "def return_tbcallbacks(model_name=None):\n",
        "    # ------ define tensorboard callback\n",
        "    if model_name is None:\n",
        "        model_name = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    else:\n",
        "        model_name = model_name\n",
        "    \n",
        "    logdir = os.path.join(\"logs\", model_name)\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir,\n",
        "                                                          histogram_freq=1,\n",
        "                                                          write_grads=True)\n",
        "    # ------ check point call backs\n",
        "    checkpointdir = os.path.join(\"checkpoints\", model_name)\n",
        "    checkpointpath = os.path.join(checkpointdir, \"{epoch:04d}.ckpt\")\n",
        "    cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpointpath,\n",
        "                                                     save_weights_only=False,\n",
        "                                                     verbose=5)\n",
        "    return tensorboard_callback, cp_callback"
      ],
      "id": "5dd916d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a172c7ef"
      },
      "source": [
        "def lr_scheduler(epoch, lr):\n",
        "    decay_rate = 0.5\n",
        "    decay_step = 30\n",
        "    if epoch % decay_step == 0 and epoch:\n",
        "        return lr * pow(decay_rate, np.floor(epoch / decay_step))\n",
        "    return lr\n",
        "callbacks = [LearningRateScheduler(lr_scheduler, verbose=1)]\n",
        "optimizer = optimizers.Adam(learning_rate=0.001)"
      ],
      "id": "a172c7ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "e9d79bbd",
        "scrolled": true,
        "outputId": "ef32e40c-322e-4877-ce72-fc4ddde28f70"
      },
      "source": [
        "res_autoencoder.compile(loss='mse', optimizer=optimizer)\n",
        "\n",
        "tensorboard_callback, cp_callback = return_tbcallbacks(model_name='model_01')\n",
        "history_model_one = res_autoencoder.fit(waves_train,\n",
        "                                         waves_train,\n",
        "                                         epochs=100, \n",
        "                                         batch_size=32, \n",
        "                                         verbose=1, \n",
        "                                         validation_split=0.2,\n",
        "                                         callbacks=[tensorboard_callback,cp_callback])"
      ],
      "id": "e9d79bbd",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-436d4285c3b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                          \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                          callbacks=[tensorboard_callback,cp_callback])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    956\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    779\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    780\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 781\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3155\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3156\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3157\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3158\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3556\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3557\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3558\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3400\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3401\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3402\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3403\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3404\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1182\u001b[0m         if x is not None)\n\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m     \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    416\u001b[0m       \u001b[0;31m# Check for any resource inputs. If we find any, we update control_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       \u001b[0;31m# and last_write_to_resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_get_resource_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0mis_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mResourceType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_ONLY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0minput_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m_get_resource_inputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    608\u001b[0m       \u001b[0;31m# TODO(srbs): An alternate would be to just compare the old and new set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# but that may not be as fast.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m       \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_acd_resource_resolvers_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;31m# Conservatively remove any resources from `reads` that are also writes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_resource_resolver\u001b[0;34m(op, resource_reads, resource_writes)\u001b[0m\n\u001b[1;32m   6611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6612\u001b[0m   \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6613\u001b[0;31m   if op.type in [\n\u001b[0m\u001b[1;32m   6614\u001b[0m       \u001b[0;34m\"DatasetToSingleElement\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetToTFRecord\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ReduceDataset\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6615\u001b[0m   ]:\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2507\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2508\u001b[0m     \u001b[0;34m\"\"\"The type of the op (e.g. `\"MatMul\"`).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2509\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationOpType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2511\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "0862039b",
        "outputId": "db6fea58-81cc-485d-b876-f277ea672aa8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "start = 1\n",
        "plt.plot(history_model_one.history['loss'][start:], label='train')\n",
        "plt.plot(history_model_one.history['val_loss'][start:], label='validation')\n",
        "plt.legend()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss, mse')\n",
        "plt.title('Autoencoder training curve')\n",
        "plt.show()"
      ],
      "id": "0862039b",
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-0caa129b8dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_model_one\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_model_one\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history_model_one' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "649d5772"
      },
      "source": [
        "### Load in pre-trained encoder model "
      ],
      "id": "649d5772"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed2c8246",
        "outputId": "094f231e-eb85-4b56-b2a1-bbf372e916a3"
      },
      "source": [
        "model_name = \"model_01\"\n",
        "epoch = 100\n",
        "path_to_model = f'./checkpoints/{model_name}/{epoch:04d}.ckpt'\n",
        "autoencoder = tf.keras.models.load_model(path_to_model)\n",
        "autoencoder.summary()"
      ],
      "id": "ed2c8246",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"autoencoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_input (InputLayer)  [(None, 600, 3)]          0         \n",
            "                                                                 \n",
            " encoder (Functional)        (None, 100)               335972    \n",
            "                                                                 \n",
            " decoder (Functional)        (None, 600, 3)            334307    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 670,279\n",
            "Trainable params: 667,975\n",
            "Non-trainable params: 2,304\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "999c786a"
      },
      "source": [
        "### Get encoder "
      ],
      "id": "999c786a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d20d6e08",
        "scrolled": true,
        "outputId": "3999e01c-fa06-4f69-ffdf-0818c594b820"
      },
      "source": [
        "encoder_input = autoencoder.get_layer('encoder_input')\n",
        "encoder = autoencoder.get_layer('encoder')\n",
        "encoder.summary()"
      ],
      "id": "d20d6e08",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_input (InputLayer)     [(None, 600, 3)]     0           []                               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 600, 32)      704         ['encoder_input[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 600, 32)     128         ['conv1d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 600, 32)      0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 600, 32)      5152        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 600, 32)     128         ['conv1d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 600, 32)      0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 600, 32)      5152        ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 600, 32)     128         ['conv1d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 600, 32)      0           ['batch_normalization_2[0][0]',  \n",
            "                                                                  'activation[0][0]']             \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 600, 32)      0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 600, 32)      5152        ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 600, 32)     128         ['conv1d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 600, 32)      0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 600, 32)      5152        ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 600, 32)     128         ['conv1d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 600, 32)      0           ['batch_normalization_4[0][0]',  \n",
            "                                                                  'activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 600, 32)      0           ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 600, 32)      5152        ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 600, 32)     128         ['conv1d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 600, 32)      0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 600, 32)      5152        ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 600, 32)     128         ['conv1d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 600, 32)      0           ['batch_normalization_6[0][0]',  \n",
            "                                                                  'activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 600, 32)      0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 300, 32)      0           ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 300, 32)      5152        ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 300, 32)     128         ['conv1d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 300, 32)      0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 300, 32)      5152        ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 300, 32)     128         ['conv1d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 300, 32)      0           ['batch_normalization_8[0][0]',  \n",
            "                                                                  'max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 300, 32)      0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 300, 32)      5152        ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 300, 32)     128         ['conv1d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 300, 32)      0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 300, 32)      5152        ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 300, 32)     128         ['conv1d_10[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 300, 32)      0           ['batch_normalization_10[0][0]', \n",
            "                                                                  'activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 300, 32)      0           ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 300, 32)      5152        ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 300, 32)     128         ['conv1d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 300, 32)      0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 300, 32)      5152        ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 300, 32)     128         ['conv1d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 300, 32)      0           ['batch_normalization_12[0][0]', \n",
            "                                                                  'activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 300, 32)      0           ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 150, 32)     0           ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 150, 32)      5152        ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 150, 32)     128         ['conv1d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 150, 32)      0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 150, 32)      5152        ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 150, 32)     128         ['conv1d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 150, 32)      0           ['batch_normalization_14[0][0]', \n",
            "                                                                  'max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 150, 32)      0           ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 150, 32)      5152        ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 150, 32)     128         ['conv1d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 150, 32)      0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 150, 32)      5152        ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 150, 32)     128         ['conv1d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 150, 32)      0           ['batch_normalization_16[0][0]', \n",
            "                                                                  'activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 150, 32)      0           ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 150, 32)      5152        ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 150, 32)     128         ['conv1d_17[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 150, 32)      0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 150, 32)      5152        ['activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 150, 32)     128         ['conv1d_18[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 150, 32)      0           ['batch_normalization_18[0][0]', \n",
            "                                                                  'activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 150, 32)      0           ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 75, 32)      0           ['activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 2400)         0           ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " latent_layer (Dense)           (None, 100)          240100      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 335,972\n",
            "Trainable params: 334,756\n",
            "Non-trainable params: 1,216\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e2cb9e1"
      },
      "source": [
        "### Data pre-processing for Barlow-twins input\n",
        "1. Strat from the grouped waveforms\n",
        "2. Pair up each of the waveforms"
      ],
      "id": "2e2cb9e1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e0141cf"
      },
      "source": [
        "train_set_bt = train_set.copy()"
      ],
      "id": "1e0141cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ab5ed60"
      },
      "source": [
        "import itertools\n",
        "def pair_up_waveform(waves):\n",
        "    \n",
        "    # ---- count number of entries\n",
        "    num_entries = waves.shape[0]\n",
        "    \n",
        "    # ---- create pair of two\n",
        "    pair_index = list(set(itertools.combinations(np.arange(num_entries),2)))\n",
        "    pair_index = np.asarray(pair_index).T\n",
        "    \n",
        "    return pair_index"
      ],
      "id": "8ab5ed60",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21224237",
        "outputId": "8e1394ac-e1a7-4a09-ba74-5b67b1033287"
      },
      "source": [
        "# ----- create indexes of pair of two\n",
        "train_set_bt[\"pair_index\"] = train_set[\"P_wave_processed\"].apply(lambda x: pair_up_waveform(x))\n",
        "train_set_bt[\"num_pairs\"] = train_set[\"num_waves\"].apply(lambda x: int(comb(x,2)))\n",
        "\n",
        "num_of_pairs = train_set_bt.num_pairs.sum()\n",
        "print(f\"total number of pairs: {num_of_pairs}\")\n",
        "num_of_events = len(train_set_bt)\n",
        "print(f\"total number of events: {num_of_events}\")"
      ],
      "id": "21224237",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total number of pairs: 10061755\n",
            "total number of events: 1034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92c9a91e"
      },
      "source": [
        "# ----- now we can populate waveform pairs to numpy arrays\n",
        "# set a max number of pair per event, if memory is an issue\n",
        "max_num_pair = 100\n",
        "total_pairs = num_of_events * max_num_pair\n",
        "\n",
        "waves_one = np.zeros((total_pairs, 600, 3))\n",
        "waves_two = np.zeros((total_pairs, 600, 3))\n",
        "\n",
        "counter = 0\n",
        "for i in range(len(train_set_bt)):\n",
        "    \n",
        "    indexes_for_waves_one = train_set_bt.iloc[i].pair_index[0]\n",
        "    indexes_for_waves_two = train_set_bt.iloc[i].pair_index[1]\n",
        "    \n",
        "    if len(indexes_for_waves_one) > max_num_pair:\n",
        "        indexes_for_waves_one = indexes_for_waves_one[0:max_num_pair]\n",
        "        indexes_for_waves_two = indexes_for_waves_one[0:max_num_pair]\n",
        "    \n",
        "    num_of_combs = len(indexes_for_waves_one) + counter\n",
        "    \n",
        "    # ---- get waves and put into the pre-instatiiated np arrays\n",
        "    waves_one[counter:num_of_combs,:,:] = train_set_bt.iloc[i][\"P_wave_processed\"][indexes_for_waves_one,:,:]\n",
        "    waves_two[counter:num_of_combs,:,:] = train_set_bt.iloc[i][\"P_wave_processed\"][indexes_for_waves_two,:,:]\n",
        "    \n",
        "    counter = num_of_combs\n",
        "    \n",
        "waves_one = waves_one[:counter,:,:]\n",
        "waves_two = waves_one[:counter,:,:]"
      ],
      "id": "92c9a91e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "031ca3ce"
      },
      "source": [
        "### Barlow-twins part starts here"
      ],
      "id": "031ca3ce"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c70565a7"
      },
      "source": [
        "Inputs:\n",
        "1. encoder : pre-trained encoder model\n",
        "2. waves_one : wave copy one\n",
        "3. waves_two : wave copy two"
      ],
      "id": "c70565a7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df496a7e"
      },
      "source": [
        "def off_diagonal(x):\n",
        "    n = tf.shape(x)[0]\n",
        "    flattened = tf.reshape(x, [-1])[:-1]\n",
        "    off_diagonals = tf.reshape(flattened, (n-1, n+1))[:, 1:]\n",
        "    return tf.reshape(off_diagonals, [-1])\n",
        "\n",
        "\n",
        "def normalize_repr(z):\n",
        "    z_norm = (z - tf.reduce_mean(z, axis=0)) / tf.math.reduce_std(z, axis=0)\n",
        "    return z_norm\n",
        "\n",
        "\n",
        "def compute_loss(z_a, z_b, lambd):\n",
        "    # Get batch size and representation dimension.\n",
        "    batch_size = tf.cast(tf.shape(z_a)[0], z_a.dtype)\n",
        "    repr_dim = tf.shape(z_a)[1]\n",
        "\n",
        "    # Normalize the representations along the batch dimension.\n",
        "    z_a_norm = normalize_repr(z_a)\n",
        "    z_b_norm = normalize_repr(z_b)\n",
        "\n",
        "    # Cross-correlation matrix.\n",
        "    c = tf.matmul(z_a_norm, z_b_norm, transpose_a=True) / batch_size\n",
        "\n",
        "    # Loss.\n",
        "    on_diag = tf.linalg.diag_part(c) + (-1)\n",
        "    on_diag = tf.reduce_sum(tf.pow(on_diag, 2))\n",
        "    off_diag = off_diagonal(c)\n",
        "    off_diag = tf.reduce_sum(tf.pow(off_diag, 2))\n",
        "    loss = on_diag + (lambd * off_diag)\n",
        "    return loss"
      ],
      "id": "df496a7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71a0a3fd"
      },
      "source": [
        "class BarlowTwins(tf.keras.Model):\n",
        "    def __init__(self, encoder, lambd=5e-3):\n",
        "        super(BarlowTwins, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.lambd = lambd\n",
        "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data.\n",
        "        ds_one, ds_two = data\n",
        "\n",
        "        # Forward pass through the encoder and predictor.\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_a, z_b = self.encoder(ds_one, training=True), self.encoder(ds_two, training=True)\n",
        "            loss = compute_loss(z_a, z_b, self.lambd) \n",
        "\n",
        "        # Compute gradients and update the parameters.\n",
        "        gradients = tape.gradient(loss, self.encoder.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.encoder.trainable_variables))\n",
        "\n",
        "        # Monitor loss.\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}"
      ],
      "id": "71a0a3fd",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c86b98f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "de24c654-72f0-4d11-daae-7944cf437405"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "STEPS_PER_EPOCH = waves_one.shape[0] // BATCH_SIZE\n",
        "TOTAL_STEPS = STEPS_PER_EPOCH * EPOCHS\n",
        "WARMUP_EPOCHS = int(EPOCHS * 0.1)\n",
        "WARMUP_STEPS = int(WARMUP_EPOCHS * STEPS_PER_EPOCH)\n",
        "\n",
        "lr_decayed_fn = lr_scheduler.WarmUpCosine(\n",
        "    learning_rate_base=1e-3,\n",
        "    total_steps=EPOCHS * STEPS_PER_EPOCH,\n",
        "    warmup_learning_rate=0.0,\n",
        "    warmup_steps=WARMUP_STEPS\n",
        ")"
      ],
      "id": "3c86b98f",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a6715e314137>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mSTEPS_PER_EPOCH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaves_one\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mTOTAL_STEPS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSTEPS_PER_EPOCH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mWARMUP_EPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'waves_one' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0196067e"
      },
      "source": [
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_decayed_fn, momentum=0.9)"
      ],
      "id": "0196067e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-tiOYyoq8l0"
      },
      "source": [
        "ssl_ds_one = tf.data.Dataset.from_tensor_slices(waves_one)\n",
        "ssl_ds_two = tf.data.Dataset.from_tensor_slices(waves_two)\n",
        "\n",
        "SEED = 0\n",
        "\n",
        "ssl_ds_one = (\n",
        "    ssl_ds_one.shuffle(1024, seed=SEED)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "ssl_ds_two = (\n",
        "    ssl_ds_two.shuffle(1024, seed=SEED)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# We then zip both of these datasets.\n",
        "ssl_ds = tf.data.Dataset.zip((ssl_ds_one, ssl_ds_two))"
      ],
      "id": "s-tiOYyoq8l0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "e948d9c0",
        "outputId": "500743dc-42bb-41fa-d6d1-e3beeff76b5e"
      },
      "source": [
        "barlow_twins = BarlowTwins(encoder)\n",
        "barlow_twins.compile(optimizer=optimizer)\n",
        "tensorboard_callback, cp_callback = return_tbcallbacks(model_name='barlow_twins')\n",
        "history = barlow_twins.fit(ssl_ds, epochs=EPOCHS, callbacks=[tensorboard_callback,cp_callback])"
      ],
      "id": "e948d9c0",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-7803f13bed1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbarlow_twins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBarlowTwins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbarlow_twins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcp_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_tbcallbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'barlow_twins'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbarlow_twins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssl_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcp_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BarlowTwins' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a45daf6",
        "outputId": "2e26b5af-09be-48cd-8ac4-f6b19a091ab5"
      },
      "source": [
        "waves_one.shape"
      ],
      "id": "6a45daf6",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(103192, 600, 3)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3600c326"
      },
      "source": [
        ""
      ],
      "id": "3600c326",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c09d7989"
      },
      "source": [
        "plt.plot(wave[:,0])\n",
        "plt.xlabel('sample 20 psp')\n",
        "plt.ylabel('normalized amplitude')\n",
        "plt.title('E-W motion')"
      ],
      "id": "c09d7989",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11a52949"
      },
      "source": [
        "plt.plot(wave[:,1])\n",
        "plt.xlabel('sample 20 psp')\n",
        "plt.ylabel('normalized amplitude')\n",
        "plt.title('N-S motion')"
      ],
      "id": "11a52949",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beef94c7"
      },
      "source": [
        "plt.plot(wave[:,2])\n",
        "plt.xlabel('sample 20 psp')\n",
        "plt.ylabel('normalized amplitude')\n",
        "plt.title('Vertical motion')"
      ],
      "id": "beef94c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e29c4d7"
      },
      "source": [
        ""
      ],
      "id": "0e29c4d7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1af1ad40"
      },
      "source": [
        ""
      ],
      "id": "1af1ad40",
      "execution_count": null,
      "outputs": []
    }
  ]
}